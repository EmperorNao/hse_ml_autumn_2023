{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Машинное обучение\n",
    "## Домашнее задание №3 - Градиентный бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** 5 декабря 2023, 23:59   \n",
    "**Штраф за опоздание:** -2 балла за каждые сутки\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Считаем производные для функций потерь (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем реализовать градиентный бустинг для 3 функций потерь:\n",
    "\n",
    "1) MSE  $L(a(x_i), y_i) = (y_i - a(x_i)) ^ 2$\n",
    "\n",
    "2) Экспоненциальная  $L(a(x_i), y_i) = exp( -a(x_i) y_i), y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "3) Логистическая  $L(a(x_i), y_i) = \\log (1 + exp( -a(x_i) y_i)), y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "где $a(x_i)$ предсказание бустинга на итом объекте. \n",
    "\n",
    "Для каждой функции потерь напишите таргет, на который будет настраиваться каждое дерево в бустинге. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. MSE  \n",
    "$L = (y_i - a_i) ^ 2$  \n",
    "$\\frac{dL}{da_i} = -2 * a_i * (y_i - a_i)$\n",
    "\n",
    "\n",
    "2. Exponential  \n",
    "$L = e^{-a_i * y_i}$  \n",
    "$\\frac{dL}{da_i} = -y_i * e^{-a_i * y_i} = - \\frac{y_i}{e^{a_i * y_i}}$\n",
    "\n",
    "\n",
    "3. Logistic  \n",
    "$L = log(1 + e^{-a_i * y_i})$  \n",
    "$\\frac{dL}{da_i} = \\frac{-y_i * e^{-a_i * y_i}}{1 + e^{-a_i * y_i}} = - \\frac{y_i}{1 + e^{a_i * y_i}}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Реализуем градиентный бустинг (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте класс градиентного бустинга для классификации. Ваша реализация бустинга должна работать по точности не более чем на 5 процентов хуже чем GradientBoostingClassifier из sklearn. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детали реализации:\n",
    "\n",
    "-- должно поддерживаться 3 функции потерь\n",
    "\n",
    "-- сами базовые алгоритмы(деревья, линейные модели и тп) реализовать не надо, просто возьмите готовые из sklearn\n",
    "\n",
    "-- в качестве функции потерь для построения одного дерева используйте MSE\n",
    "\n",
    "-- шаг в бустинге можно не подбирать, можно брать константный\n",
    "\n",
    "-- можно брать разные модели в качестве инициализации бустинга\n",
    "\n",
    "-- должны поддерживаться следующие параметры:\n",
    "\n",
    "а) число итераций\n",
    "б) размер шага\n",
    "в) процент случайных фичей при построении одного дерева\n",
    "д) процент случайных объектов при построении одного дерева\n",
    "е) параметры базового алгоритма (передавайте через **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1, stratify=wine.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Ansemble:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "        \n",
    "    def add_model(self, model, weight):\n",
    "        self.models.append((weight, model))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        result = 0\n",
    "        for weight, model in self.models:\n",
    "            predict = model.predict(X)\n",
    "            if len(predict.shape) == 1:\n",
    "                predict = np.array([1 - predict, predict]).T\n",
    "            result += weight * predict\n",
    "        return result\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "\n",
    "class ConstantModel:\n",
    "    \n",
    "    def __init__(self, c):\n",
    "        self.c = c\n",
    "    def predict(self, X):\n",
    "        return np.full((X.shape[0]), self.c)\n",
    "    \n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "class FewClassesTree:\n",
    "    \n",
    "    def __init__(self, trees, features=None):\n",
    "        self.trees = trees\n",
    "        self.features = features\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_sampled = X\n",
    "        if self.features is not None:\n",
    "            X_sampled = X[:, self.features]\n",
    "        predicts = []\n",
    "        for tree in self.trees:\n",
    "            predicts.append(tree.predict(X_sampled))\n",
    "        return np.array(predicts).T\n",
    "    \n",
    "\n",
    "class MyGradientBoostingClassifier:\n",
    "\n",
    "    def __init__(self, loss='mse', learning_rate=0.1, n_estimators=100, subsample=None, colsample=1.0, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        loss -- один из 3 лоссов:\n",
    "        learning_rate -- шаг бустинга\n",
    "        n_estimators -- число итераций\n",
    "        colsample -- процент рандомных признаков при обучнеии одного алгоритма\n",
    "        colsample -- процент рандомных объектов при обучнеии одного алгоритма\n",
    "        args, kwargs -- параметры  базовых моделей\n",
    "        \"\"\"\n",
    "        \n",
    "        assert loss in ['mse', 'exponential', 'log_loss']\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.colsample = colsample\n",
    "        self.subsample = subsample\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "        self.final_model = None\n",
    "        self.classes = None\n",
    "        \n",
    "    def _grad(self, y_true, y_pred):\n",
    "        \n",
    "        if self.loss == 'mse':\n",
    "            return -2 * y_pred * (y_true - y_pred)\n",
    "        \n",
    "        if self.loss == 'exponential':\n",
    "            return -y_true / np.exp(y_true * y_pred)\n",
    "            \n",
    "        if self.loss == 'log_loss':\n",
    "            return -y_true / np.exp(1 + y_true * y_pred)\n",
    "    \n",
    "    def fit(self, X, y, base_model=DecisionTreeRegressor, init_model=None):\n",
    "        \"\"\"\n",
    "        X -- объекты для обучения:\n",
    "        y -- таргеты для обучения\n",
    "        base_model -- класс базовых моделей, например sklearn.tree.DecisionTreeRegressor\n",
    "        init_model -- класс для первой модели, если None то берем константу (только для посл задания)\n",
    "        \"\"\"\n",
    "            \n",
    "        self.classes, counts = np.unique(y, return_counts=True)\n",
    "        probas = counts / np.sum(counts)\n",
    "        \n",
    "        y_ohe = np.eye(len(self.classes))[y]\n",
    "        \n",
    "        self.final_model = Ansemble()\n",
    "        \n",
    "        if init_model is not None:\n",
    "            init_model.fit(X, y)\n",
    "            self.final_model.add_model(init_model, 1.0)\n",
    "            \n",
    "        else:\n",
    "            trees = []\n",
    "            for k in self.classes:\n",
    "                trees.append(ConstantModel(probas[k]))\n",
    "\n",
    "            self.final_model.add_model(FewClassesTree(trees), 1.0)\n",
    "        \n",
    "        if self.subsample:\n",
    "            n_subsamples = int(self.subsample * X.shape[0])        \n",
    "        n_colsamples = int(self.colsample * X.shape[1])\n",
    "        \n",
    "        last_outputs = self.final_model(X)\n",
    "        for n_iter in range(self.n_estimators):\n",
    "            antigrad = -self._grad(y_ohe, last_outputs)\n",
    "            \n",
    "            current_model = None\n",
    "            trees = []\n",
    "            \n",
    "            if self.subsample:\n",
    "                subsamples = np.random.choice(np.arange(X.shape[0]), size=n_subsamples, replace=True)\n",
    "            else:\n",
    "                subsamples = np.arange(X.shape[0])\n",
    "                \n",
    "            colsamples = np.random.choice(np.arange(X.shape[1]), size=n_colsamples, replace=False)\n",
    "            val, cnts = np.unique(subsamples, return_counts=True)\n",
    "            X_sampled = X[subsamples, :][:, colsamples]\n",
    "            y_sampled = antigrad[subsamples, :]\n",
    "            for k in self.classes:\n",
    "                \n",
    "                new_estimator = base_model(*self.args, **self.kwargs)\n",
    "                new_estimator.fit(X_sampled, y_sampled[:, k])\n",
    "                trees.append(new_estimator)\n",
    "            \n",
    "            current_model = FewClassesTree(trees, colsamples)\n",
    "            \n",
    "            weight = self.learning_rate\n",
    "            self.final_model.add_model(current_model, weight)\n",
    "            \n",
    "            last_outputs += weight * current_model.predict(X)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        assert self.final_model, \"MyGradientBoostingClassifier wasn't fitted\"\n",
    "        return softmax(self.final_model(X))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyGradientBoostingClassifier()\n",
    "clf = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1, stratify=wine.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444444444444444\n",
      "0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "my_clf.fit(X_train, y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(y_pred=clf.predict(X_test), y_true=y_test))\n",
    "print(accuracy_score(y_pred=my_clf.predict(X_test), y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подбираем параметры (2 балла)\n",
    "\n",
    "Давайте попробуем применить Ваш бустинг для предсказаний цены домов в Калифорнии. Чтобы можно было попробовтаь разные функции потерь, переведем по порогу таргет в 2 класса: дорогие и дешевые дома."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задании нужно\n",
    "\n",
    "1) Построить график точности в зависимости от числа итераций на валидации.\n",
    "\n",
    "2) Подобрать оптимальные параметры Вашего бустинга на валидации. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "X, y = fetch_california_housing(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8) (20640,)\n"
     ]
    }
   ],
   "source": [
    "# Превращаем регрессию в классификацию\n",
    "y = (y > 2.0).astype(int)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [01:52<00:00, 22.60s/it]\n"
     ]
    }
   ],
   "source": [
    "metrics = {'train': [], 'val': [], 'n_estim': []}\n",
    "\n",
    "for n_estim in tqdm([1, 10, 50, 100, 250]):\n",
    "    my_clf = MyGradientBoostingClassifier(n_estimators=n_estim)\n",
    "    my_clf.fit(X_train, y_train)\n",
    "    \n",
    "    metrics['n_estim'].append(n_estim)\n",
    "    metrics['train'].append(accuracy_score(y_true=y_train, y_pred=my_clf.predict(X_train)))\n",
    "    metrics['val'].append(accuracy_score(y_true=y_val, y_pred=my_clf.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "plt.title(\"Accuracy by iter\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(metrics['n_estim'], metrics['train'], label='Train')\n",
    "plt.plot(metrics['n_estim'], metrics['val'], label='Val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: loss = mse, lr = 1, max_depth = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_298856/2947093488.py:74: RuntimeWarning: overflow encountered in multiply\n",
      "  return -2 * y_pred * (y_true - y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip: Input y contains infinity or a value too large for dtype('float64').\n",
      "Params: loss = mse, lr = 1, max_depth = 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_298856/2947093488.py:74: RuntimeWarning: overflow encountered in multiply\n",
      "  return -2 * y_pred * (y_true - y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip: Input y contains infinity or a value too large for dtype('float64').\n",
      "Params: loss = mse, lr = 1, max_depth = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_298856/2947093488.py:74: RuntimeWarning: overflow encountered in multiply\n",
      "  return -2 * y_pred * (y_true - y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip: Input y contains infinity or a value too large for dtype('float64').\n",
      "Params: loss = mse, lr = 0.1, max_depth = 3\n",
      "Train acc: 0.8863250968992248\n",
      "Val acc: 0.8844476744186046\n",
      "Params: loss = mse, lr = 0.1, max_depth = 6\n",
      "Train acc: 0.9385901162790697\n",
      "Val acc: 0.8989825581395349\n",
      "Params: loss = mse, lr = 0.1, max_depth = 10\n",
      "Train acc: 0.9983648255813954\n",
      "Val acc: 0.9026162790697675\n",
      "Params: loss = mse, lr = 0.01, max_depth = 3\n",
      "Train acc: 0.8231589147286822\n",
      "Val acc: 0.8304263565891473\n",
      "Params: loss = mse, lr = 0.01, max_depth = 6\n",
      "Train acc: 0.8815406976744186\n",
      "Val acc: 0.8757267441860465\n",
      "Params: loss = mse, lr = 0.01, max_depth = 10\n",
      "Train acc: 0.9522165697674418\n",
      "Val acc: 0.8900193798449613\n",
      "Params: loss = log_loss, lr = 1, max_depth = 3\n",
      "Train acc: 0.8462330426356589\n",
      "Val acc: 0.8524709302325582\n",
      "Params: loss = log_loss, lr = 1, max_depth = 6\n",
      "Train acc: 0.8886264534883721\n",
      "Val acc: 0.8810562015503876\n",
      "Params: loss = log_loss, lr = 1, max_depth = 10\n",
      "Train acc: 0.9609375\n",
      "Val acc: 0.8968023255813954\n",
      "Params: loss = log_loss, lr = 0.1, max_depth = 3\n",
      "Train acc: 0.8171632751937985\n",
      "Val acc: 0.8224321705426356\n",
      "Params: loss = log_loss, lr = 0.1, max_depth = 6\n",
      "Train acc: 0.8731225775193798\n",
      "Val acc: 0.8708817829457365\n",
      "Params: loss = log_loss, lr = 0.1, max_depth = 10\n",
      "Train acc: 0.9439801356589147\n",
      "Val acc: 0.8880813953488372\n",
      "Params: loss = log_loss, lr = 0.01, max_depth = 3\n",
      "Train acc: 0.6587330426356589\n",
      "Val acc: 0.657218992248062\n",
      "Params: loss = log_loss, lr = 0.01, max_depth = 6\n",
      "Train acc: 0.7059714147286822\n",
      "Val acc: 0.6981589147286822\n",
      "Params: loss = log_loss, lr = 0.01, max_depth = 10\n",
      "Train acc: 0.7780402131782945\n",
      "Val acc: 0.7449127906976745\n",
      "Params: loss = exponential, lr = 1, max_depth = 3\n",
      "Train acc: 0.8513202519379846\n",
      "Val acc: 0.8585271317829457\n",
      "Params: loss = exponential, lr = 1, max_depth = 6\n",
      "Train acc: 0.8949249031007752\n",
      "Val acc: 0.8866279069767442\n",
      "Params: loss = exponential, lr = 1, max_depth = 10\n",
      "Train acc: 0.9649345930232558\n",
      "Val acc: 0.8905038759689923\n",
      "Params: loss = exponential, lr = 0.1, max_depth = 3\n",
      "Train acc: 0.829578488372093\n",
      "Val acc: 0.8391472868217055\n",
      "Params: loss = exponential, lr = 0.1, max_depth = 6\n",
      "Train acc: 0.880390019379845\n",
      "Val acc: 0.8757267441860465\n",
      "Params: loss = exponential, lr = 0.1, max_depth = 10\n",
      "Train acc: 0.9527616279069767\n",
      "Val acc: 0.8914728682170543\n",
      "Params: loss = exponential, lr = 0.01, max_depth = 3\n",
      "Train acc: 0.7678657945736435\n",
      "Val acc: 0.7718023255813954\n",
      "Params: loss = exponential, lr = 0.01, max_depth = 6\n",
      "Train acc: 0.8467175387596899\n",
      "Val acc: 0.841812015503876\n",
      "Params: loss = exponential, lr = 0.01, max_depth = 10\n",
      "Train acc: 0.9166666666666666\n",
      "Val acc: 0.8728197674418605\n"
     ]
    }
   ],
   "source": [
    "## Не наследуемся от склерновского класса, поэтому GridSearch не заходит\n",
    "\n",
    "for loss in ['mse', 'log_loss', 'exponential']:\n",
    "    for lr in [1, 0.1, 0.01]:\n",
    "        for max_depth in [3, 6, 10]:\n",
    "            print(f\"Params: loss = {loss}, lr = {lr}, max_depth = {max_depth}\")  \n",
    "            try:\n",
    "                model = MyGradientBoostingClassifier(loss, lr, max_depth=max_depth)\n",
    "                model.fit(X_train, y_train)\n",
    "                print(f\"Train acc: {accuracy_score(y_true=y_train, y_pred=model.predict(X_train))}\")\n",
    "                print(f\"Val acc: {accuracy_score(y_true=y_val, y_pred=model.predict(X_val))}\")                            \n",
    "            except Exception as e:\n",
    "                print(f\"skip: {e}\")\n",
    "#             model = GradientBoostingClassifier(loss=loss, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучше параметры - loss = mse, lr = 0.1, max_depth = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший скор = 0.9026162790697675"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BooBag BagBoo (1 балл)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем объединить бустинг и бэгинг. Давайте\n",
    "\n",
    "1) в качестве базовой модели брать не дерево решений, а случайный лес (из sklearn)\n",
    "\n",
    "2) обучать N бустингов на бустрапированной выборке, а затем предикт усреднять"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте обе этих стратегии на данных из прошлого задания. Получилось ли улучшить качество? Почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.5780644379844961\n",
      "Val acc: 0.5780038759689923\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = MyGradientBoostingClassifier('mse', 0.001, 10)\n",
    "model.fit(X_train, y_train, RandomForestRegressor)\n",
    "\n",
    "print(f\"Train acc: {accuracy_score(y_true=y_train, y_pred=model.predict(X_train))}\")\n",
    "print(f\"Val acc: {accuracy_score(y_true=y_val, y_pred=model.predict(X_val))}\")               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговые метрики c MSE не улучшаются, при этом антиградиент изначально маленький и растёт, даже с маленьким lr - значит мы ухудшаем предсказания новыми моделями, потому что базовая модель итак очень хорошая."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11929756 0.10179053]\n",
      "[0.09866147 0.08212182]\n",
      "[0.08442708 0.06914598]\n",
      "[0.07455437 0.0605479 ]\n",
      "[0.0666824  0.05378499]\n",
      "[0.06007948 0.04812594]\n",
      "[0.05481836 0.04372027]\n",
      "[0.05026897 0.03994025]\n",
      "[0.04666131 0.03695833]\n",
      "[0.04349653 0.03438683]\n",
      "[0.04072631 0.03215391]\n",
      "[0.0382805  0.03016895]\n",
      "[0.03610332 0.02840562]\n",
      "[0.03411075 0.02678446]\n",
      "[0.03239784 0.02543004]\n",
      "[0.03083575 0.02418604]\n",
      "[0.02938004 0.02302049]\n",
      "[0.02812169 0.02203289]\n",
      "[0.02694169 0.02110211]\n",
      "[0.02585592 0.02025136]\n",
      "Train acc: 0.9986676356589147\n",
      "Val acc: 0.8982558139534884\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = MyGradientBoostingClassifier('log_loss', 1, 20)\n",
    "model.fit(X_train, y_train, RandomForestRegressor)\n",
    "\n",
    "print(f\"Train acc: {accuracy_score(y_true=y_train, y_pred=model.predict(X_train))}\")\n",
    "print(f\"Val acc: {accuracy_score(y_true=y_val, y_pred=model.predict(X_val))}\")               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для log_loss антиградиент падает, но при этом итоговые метрики улучшаются совсем немного"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.998062015503876\n",
      "Val acc: 0.8929263565891473\n",
      "Train acc: 0.9976986434108527\n",
      "Val acc: 0.8951065891472868\n",
      "Train acc: 0.9977592054263565\n",
      "Val acc: 0.8934108527131783\n",
      "Train acc: 0.9980014534883721\n",
      "Val acc: 0.8914728682170543\n",
      "Train acc: 0.9976986434108527\n",
      "Val acc: 0.8902616279069767\n",
      "Ansemble metrics:\n",
      "Train acc: 0.9986070736434108\n",
      "Val acc: 0.8943798449612403\n"
     ]
    }
   ],
   "source": [
    "ansemble = Ansemble()\n",
    "\n",
    "nmodels = 5\n",
    "for _ in range(nmodels):\n",
    "    model = MyGradientBoostingClassifier('mse', 0.1, subsample=1.0, max_depth = 10)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Train acc: {accuracy_score(y_true=y_train, y_pred=model.predict(X_train))}\")\n",
    "    print(f\"Val acc: {accuracy_score(y_true=y_val, y_pred=model.predict(X_val))}\")               \n",
    "    \n",
    "    ansemble.add_model(model, 1.0 / nmodels)   \n",
    "    \n",
    "print(\"Ansemble metrics:\")\n",
    "print(f\"Train acc: {accuracy_score(y_true=y_train, y_pred=(ansemble.predict(X_train)[:, 1] > 0.5).astype(int))}\")\n",
    "print(f\"Val acc: {accuracy_score(y_true=y_val, y_pred=(ansemble.predict(X_val)[:, 1] > 0.5).astype(int))}\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для MSE метрики не улучшаются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.9543968023255814\n",
      "Val acc: 0.8844476744186046\n",
      "Train acc: 0.9538517441860465\n",
      "Val acc: 0.8793604651162791\n",
      "Train acc: 0.9547601744186046\n",
      "Val acc: 0.8808139534883721\n",
      "Train acc: 0.9530038759689923\n",
      "Val acc: 0.8781492248062015\n",
      "Train acc: 0.9524588178294574\n",
      "Val acc: 0.8822674418604651\n",
      "Ansemble metrics:\n",
      "Train acc: 0.9544573643410853\n",
      "Val acc: 0.8798449612403101\n"
     ]
    }
   ],
   "source": [
    "ansemble = Ansemble()\n",
    "\n",
    "nmodels = 5\n",
    "for _ in range(nmodels):\n",
    "    model = MyGradientBoostingClassifier('log_loss', 1, 100, subsample=1.0, max_depth = 10)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Train acc: {accuracy_score(y_true=y_train, y_pred=model.predict(X_train))}\")\n",
    "    print(f\"Val acc: {accuracy_score(y_true=y_val, y_pred=model.predict(X_val))}\")               \n",
    "    \n",
    "    ansemble.add_model(model, 1.0 / nmodels)   \n",
    "    \n",
    "print(\"Ansemble metrics:\")\n",
    "print(f\"Train acc: {accuracy_score(y_true=y_train, y_pred=(ansemble.predict(X_train)[:, 1] > 0.5).astype(int))}\")\n",
    "print(f\"Val acc: {accuracy_score(y_true=y_val, y_pred=(ansemble.predict(X_val)[:, 1] > 0.5).astype(int))}\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для log_loss метрики также не улучшаются"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Умная инициализация (1 балл)\n",
    "\n",
    "Попробуйте брать в качестве инициализации бустинга не константу, а какой-то алгоритм и уже от его предикта стартовать итерации бустинга. Попробуйте разные модели из sklearn: линейные модели, рандом форест, svm..\n",
    "\n",
    "Получилось ли улучшить качество? Почему?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVR, SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init model: <class 'sklearn.linear_model._base.LinearRegression'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_298856/3755535427.py:77: RuntimeWarning: overflow encountered in multiply\n",
      "  return -2 * y_pred * (y_true - y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: Input y contains infinity or a value too large for dtype('float64').\n",
      "Init model: <class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
      "Train acc: 1.0\n",
      "Val acc: 0.8909883720930233\n",
      "Init model: <class 'sklearn.svm._classes.SVR'>\n",
      "Train acc: 0.8634932170542635\n",
      "Val acc: 0.8197674418604651\n"
     ]
    }
   ],
   "source": [
    "for init_model in [LinearRegression, RandomForestRegressor, SVR]:\n",
    "    print(f\"Init model: {init_model}\")\n",
    "    try:\n",
    "        model = MyGradientBoostingClassifier('mse', 0.01, max_depth = 10)\n",
    "        model.fit(X_train, y_train, init_model=init_model())\n",
    "\n",
    "        print(f\"Train acc: {accuracy_score(y_true=y_train, y_pred=model.predict(X_train))}\")\n",
    "        print(f\"Val acc: {accuracy_score(y_true=y_val, y_pred=model.predict(X_val))}\")               \n",
    "    except Exception as e:\n",
    "        print(f\"exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Почему-то не получается улучшить качество. Может быть, требуется более качественная настройка каждого из базовых алгоритмов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фидбек (бесценно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Какие аспекты обучения  ансамблей Вам показались непонятными? Какое место стоит дополнительно объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На лекции ничего не разбиралось про многоклассовую классификацию в бустинге, вообще ни слова - пришлось читать sklearn и разбираться самому. Прикольно, решение логичное, но можно было хотя бы вкратце описать алгоритм, чтобы было не так непонятно в начале"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
